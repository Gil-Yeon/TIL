{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_test=pd.read_excel(\"C:\\\\Users\\\\spa84\\\\감성분석\\\\review_merge\\\\review_merge1.xlsx\")\n",
    "review_test.rename(columns={review_test.columns[0]:'rating',review_test.columns[1]:'text'},inplace=True)\n",
    "review_test.drop_duplicates(subset=['text'],inplace=True)#데이터 수를 줄인 test 데이터에요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def korean_extract(text):\n",
    "    korean = re.compile('[^ ㄱ-ㅣ 가-힣]')#한글,공백 제외 제거\n",
    "    result =korean.sub('',text)#text에 적용\n",
    "    return result\n",
    "\n",
    "stopwords=pd.read_csv(\"C:\\\\Users\\\\spa84\\\\감성분석\\\\불용어.csv\",encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "nouns = okt.nouns(korean_extract(review_test['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    6680\n",
       "0     1099\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rating_sort(rating):#일단 4,5점이 그래프 보면 많으니 이렇게 나눠보니까 차이가 확실히 많긴 많음\n",
    "    if rating >3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "review_test['y']=review_test['rating'].apply(lambda x: rating_sort(x))\n",
    "review_test['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    17034\n",
       "0      2428\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_train=pd.read_excel(\"C:\\\\Users\\\\spa84\\\\감성분석\\\\review_merge\\\\review_merge.xlsx\")\n",
    "review_train.rename(columns={review_train.columns[0]:'rating',review_train.columns[1]:'text'},inplace=True)\n",
    "review_train.drop_duplicates(subset=['text'],inplace=True)\n",
    "\n",
    "def korean_extract(text):\n",
    "    korean = re.compile('[^ ㄱ-ㅣ 가-힣]')#한글,공백 제외 제거\n",
    "    result =korean.sub('',text)#text에 적용\n",
    "    return result\n",
    "\n",
    "stopwords=pd.read_csv(\"C:\\\\Users\\\\spa84\\\\감성분석\\\\불용어.csv\",encoding=\"cp949\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(korean_extract(review_train['text'][0]))\n",
    "\n",
    "def rating_sort(rating):\n",
    "    if rating >3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "review_train['y']=review_train['rating'].apply(lambda x: rating_sort(x))\n",
    "review_train['y'].value_counts() #아까 테스트 리뷰에 했던걸 똑같이 train 데이터에도 해주는 거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7779/7779 [00:45<00:00, 170.38it/s]\n"
     ]
    }
   ],
   "source": [
    "a_test=[]\n",
    "for sentence in tqdm(review_test['text']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True)\n",
    "    nouns = [x for x in tokenized_sentence if len(x) > 1]  \n",
    "    nouns = [x for x in tokenized_sentence if x not in stopwords]\n",
    "    a_test.append(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 19462/19462 [02:11<00:00, 148.23it/s]\n"
     ]
    }
   ],
   "source": [
    "a_train=[]\n",
    "for sentence in tqdm(review_train['text']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True)\n",
    "    nouns = [x for x in tokenized_sentence if len(x) > 1]  \n",
    "    nouns = [x for x in tokenized_sentence if x not in stopwords]\n",
    "    a_train.append(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=Tokenizer()\n",
    "token.fit_on_texts(a_train)#단어 들을 토큰화 해서 단어마다 숫자를 대응시켜서 딕셔너리 형태로 만들어주려고 그러면 나중에 빈도수가 높은 단어부터 차례대로 고유 번호를 부여 받을 수 있어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 8102\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 4606\n",
      "단어 집합에서 희귀 단어의 비율: 56.85016045420884\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.3825884045900654\n"
     ]
    }
   ],
   "source": [
    "threshold = 3 #희귀 단어 등장 빈도 비율을 보고 3을 2로 바꿔서도 해보고 해서 정하면 될것 같아\n",
    "rare_cnt = 0 \n",
    "total_freq = 0\n",
    "rare_freq = 0\n",
    "total_cnt = len(token.word_index)\n",
    "\n",
    "for key, value in token.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = total_cnt - rare_cnt + 1#이렇게 해서 만약 희귀단어 빈도 비율이 낮다면 삭제를 해주는 방향으로 가자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(vocab_size) \n",
    "token.fit_on_texts(a_train)\n",
    "a_train = token.texts_to_sequences(a_train)\n",
    "a_test = token.texts_to_sequences(a_test)#이거는 텍스트를 시퀀스로 변환하는건데 아까 위에서 부여 받은 숫자를 시퀀스(행렬?) 형태로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(review_train['y'])\n",
    "y_test = np.array(review_test['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(a_train) if len(sentence) < 1]#위에서 빈도 낮은걸 지워주면 빈 데이터니까 그걸 지워주는 과정이야"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 701\n",
      "리뷰의 평균 길이 : 20.564279108005344\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in a_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, a_train))/len(a_train))\n",
    "plt.hist([len(s) for s in a_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()#이거를 하는 이유는 지금 우리가 가지고 있는 리뷰의 길이가 다 다르잖아 그래서 나중에 이걸 처리하려면 다 같은 길이어야지 한꺼번에 묶어서 처리 할 수 있거든 그래서 길이들을 확인해보고 맞춰주기 위해서 보는거야"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 중 길이가 100 이하인 샘플의 비율: 98.23245298530469\n"
     ]
    }
   ],
   "source": [
    "def review_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('리뷰 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100)) #\n",
    "\n",
    "max_len=100\n",
    "#위에 그래프를 보고 어느정도의 숫자를 넣어서 train데이터 중 몇%가 우리가 넣은 숫자 이하의 길이를 갖나 판단해서 너무 길게 말고 대신 짤리는 리뷰가 없도록 설정을 해주는거야\n",
    "review_len(max_len,a_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train = pad_sequences(a_train, maxlen = max_len)\n",
    "a_test = pad_sequences(a_test, maxlen = max_len)#이제 우리가 정한 최대길이로 모든 데이터의 길이를 맞추는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 #차원을 100으로 잡았어 임베딩이니까 이제 1,0만 있는게 아니라 모든 실수로서 저장이 가능해짐\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(LSTM(hidden_units))#장단기 메모리 이고 불필요한건 지우고 필요한것만 저장해주는 거야\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4) #검증 데이터 손실나면 4회에서 학습을 종료해줌\n",
    "mc = ModelCheckpoint('best_model', monitor='val_acc', mode='max', verbose=1, save_best_only=True)#이전보다 정확도가 좋아지는 것만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "244/244 [==============================] - 24s 95ms/step - loss: -217.0859 - acc: 3.8538e-04 - val_loss: -410.0443 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.00000, saving model to best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "244/244 [==============================] - 24s 100ms/step - loss: -455.4244 - acc: 0.0000e+00 - val_loss: -692.9453 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.00000\n",
      "Epoch 3/15\n",
      "244/244 [==============================] - 25s 101ms/step - loss: -688.9431 - acc: 0.0000e+00 - val_loss: -975.4815 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.00000\n",
      "Epoch 4/15\n",
      "244/244 [==============================] - 26s 108ms/step - loss: -922.5806 - acc: 0.0000e+00 - val_loss: -1258.5635 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.00000\n",
      "Epoch 5/15\n",
      "244/244 [==============================] - 24s 98ms/step - loss: -1156.3320 - acc: 0.0000e+00 - val_loss: -1541.1093 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.00000\n",
      "Epoch 6/15\n",
      "244/244 [==============================] - 24s 100ms/step - loss: -1390.1401 - acc: 0.0000e+00 - val_loss: -1824.0216 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.00000\n",
      "Epoch 7/15\n",
      "244/244 [==============================] - 25s 104ms/step - loss: -1624.0614 - acc: 0.0000e+00 - val_loss: -2106.7551 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.00000\n",
      "Epoch 8/15\n",
      "244/244 [==============================] - 24s 100ms/step - loss: -1857.7966 - acc: 0.0000e+00 - val_loss: -2389.4199 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.00000\n",
      "Epoch 9/15\n",
      "244/244 [==============================] - 26s 108ms/step - loss: -2091.6663 - acc: 0.0000e+00 - val_loss: -2672.2605 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.00000\n",
      "Epoch 10/15\n",
      "244/244 [==============================] - 25s 104ms/step - loss: -2325.3000 - acc: 0.0000e+00 - val_loss: -2955.1350 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.00000\n",
      "Epoch 11/15\n",
      "244/244 [==============================] - 25s 102ms/step - loss: -2558.5859 - acc: 0.0000e+00 - val_loss: -3237.7197 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.00000\n",
      "Epoch 12/15\n",
      "244/244 [==============================] - 25s 102ms/step - loss: -2792.6414 - acc: 0.0000e+00 - val_loss: -3520.1045 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.00000\n",
      "Epoch 13/15\n",
      "244/244 [==============================] - 24s 99ms/step - loss: -3026.0376 - acc: 0.0000e+00 - val_loss: -3803.0159 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.00000\n",
      "Epoch 14/15\n",
      "244/244 [==============================] - 24s 100ms/step - loss: -3259.9607 - acc: 0.0000e+00 - val_loss: -4085.8782 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.00000\n",
      "Epoch 15/15\n",
      "244/244 [==============================] - 24s 99ms/step - loss: -3493.7844 - acc: 0.0000e+00 - val_loss: -4368.2490 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.00000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(a_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)#train 데이터에서 20%만 검증데이터로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 6s 23ms/step - loss: -345.6776 - acc: 0.0000e+00\n",
      "\n",
      " 테스트 정확도: 0.0000\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(a_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_predict(new_sentence):\n",
    "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "  new_sentence = okt.morphs(new_sentence, stem=True) \n",
    "  new_sentence = [word for word in new_sentence if not word in stopwords]\n",
    "  encoded = token.texts_to_sequences([new_sentence])\n",
    "  pad_new = pad_sequences(encoded, maxlen = max_len) \n",
    "  score = float(loaded_model.predict(pad_new))\n",
    "  if(score > 0.5):\n",
    "    print(\"{:.2f}% 긍정 리뷰.\\n\".format(score * 100))\n",
    "  else:\n",
    "    print(\"{:.2f}% 부정 리뷰.\\n\".format((1 - score) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
